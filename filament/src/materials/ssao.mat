material {
    name : ssao,
    parameters : [
        {
            type : sampler2d,
            name : depth,
            precision: high
        },
        {
            type : float,
            name : radius
        },
        {
            type : float,
            name : invRadiusSquared
        },
        {
            type : float,
            name : projectionScaleRadius
        },
        {
            type : float,
            name : bias
        },
        {
            type : float,
            name : power
        }
    ],
    variables : [
         vertex
    ],
    vertexDomain : device,
    depthWrite : false,
    depthCulling : false,
    shadingModel : unlit,
    variantFilter : [ skinning ],
    culling: none
}

vertex {
    void materialVertex(inout MaterialVertexInputs material) {
        // far-plane in view space
        vec4 position = getPosition(); // clip-space
        material.vertex.xy = (position.xy * 0.5 + 0.5);
        material.vertex.zw = position.xy;
    }
}

fragment {
    #define NOISE_NONE      0
    #define NOISE_PATTERN   1
    #define NOISE_RANDOM    2
    #define NOISE_TYPE      NOISE_PATTERN

    const uint kSphereSampleCount = 16u;
    const vec3 kSphereSamples[16] = vec3[](
        vec3(-1.60954e-06,3.93118e-07,1.51895e-06), vec3(-0.0950889,0.00458908,-0.0312535),
        vec3(0.0151799,-0.0255864,0.00376453), vec3(0.0734262,0.0218022,0.0027781),
        vec3(0.0945874,0.0432184,0.0891475), vec3(-0.00950861,0.0513686,0.019673),
        vec3(0.139973,-0.101685,0.10857), vec3(-0.103804,0.219853,-0.0430164),
        vec3(0.00484053,-0.0339878,0.0941868), vec3(0.0280114,0.0584662,-0.25711),
        vec3(-0.0510306,0.074993,0.259843), vec3(0.118822,-0.186537,-0.134192),
        vec3(0.0639494,-0.0948936,-0.072683), vec3(0.108176,0.327108,-0.254058),
        vec3(-0.0471796,0.21918,0.263895), vec3(-0.407709,0.240834,-0.200352)
    );

    const uint kNoiseSampleCount = 16u;
    const vec3 kNoiseSamples[kNoiseSampleCount] = vec3[](
        vec3(-0.0782473,-0.749924,-0.65688), vec3(-0.572319,-0.102379,-0.813615), vec3(0.0486527,-0.380791,0.92338), vec3(0.281202,-0.656664,-0.699799),
        vec3(0.711911,-0.235841,-0.661485), vec3(-0.445893,0.611063,0.65405), vec3(-0.703598,0.674837,0.222587), vec3(0.768236,0.507457,0.390257),
        vec3(-0.670286,-0.470387,0.57398), vec3(0.199235,0.849336,-0.488808), vec3(-0.768068,-0.583633,-0.26352), vec3(-0.89733,0.328853,0.294372),
        vec3(-0.57093,-0.531056,-0.626114), vec3(0.699014,0.0632826,-0.712303), vec3(0.207495,0.976129,-0.0641723), vec3(-0.0609008,-0.869738,-0.489742)
    );

    // random number between 0 and 1
    float random(highp vec2 n) {
        n  = fract(n * vec2(5.3987, 5.4421));
        n += dot(n.yx, n.xy + vec2(21.5351, 14.3137));
        highp float xy = n.x * n.y;
        // compute in [0..2[ and remap to [0.0..1.0[
        return fract(xy * 95.4307) + fract(xy * 75.04961) * 0.5;
    }

    // noise vector between -1 and 1
    vec3 getNoise(const vec2 uv) {
        #if NOISE_TYPE == NOISE_RANDOM
            return normalize(2.0 * vec3(random(uv), random(uv * 2.0), random(uv * 4.0)) - vec3(1.0));
        #elif NOISE_TYPE == NOISE_PATTERN
            uint ix = uint(gl_FragCoord.x) & 3u;
            uint iy = uint(gl_FragCoord.y) & 3u;
            return kNoiseSamples[ix + iy * 4u];
        #else
            return vec3(0.0);
        #endif
    }

    highp vec3 computeViewSpaceNormalNotNormalized(const highp vec3 position) {
        highp vec3 dpdx = dFdx(position);
        highp vec3 dpdy = dFdy(position);
        return cross(dpdx, dpdy);
    }

    highp float linearizeDepth(highp float depth) {
        highp mat4 projection = getClipFromViewMatrix();
        highp float z = depth * 2.0 - 1.0; // depth in clip space
        return -projection[3].z / (z + projection[2].z);
    }

    highp float sampleDepthLinear(const vec2 uv) {
        return linearizeDepth(texture(materialParams_depth, uv, 0.0).r);
    }

    highp vec3 computeViewSpacePositionFromDepth(in vec2 p, highp float linearDepth) {
        highp mat4 invProjection = getViewFromClipMatrix();
        p.x *= invProjection[0].x;
        p.y *= invProjection[1].y;
        return vec3(p * -linearDepth, linearDepth);
    }

    // Ambient Occlusion, largely inspired from:
    // Hemisphere Crysis-style SSAO. See "Screen Space Ambient Occlusion" by John Chapman

    float computeAmbientOcclusionSSAO(const highp vec3 origin, const vec3 normal, const vec3 noise, const vec3 sphereSample) {
        highp mat4 projection = getClipFromViewMatrix();
        float radius = materialParams.radius;
        float bias = materialParams.bias;

        vec3 r = sphereSample * radius;
        r = reflect(r, noise);
        r = sign(dot(r, normal)) * r;
        highp vec3 samplePos = origin + r;

        highp vec4 samplePosScreen = projection * vec4(samplePos, 1.0);
        samplePosScreen.xy = samplePosScreen.xy * (0.5 / samplePosScreen.w) + 0.5;
        highp float occlusionDepth = sampleDepthLinear(samplePosScreen.xy);
        // smoothstep() optimized for range 0 to 1
        float t = saturate(radius / abs(origin.z - occlusionDepth));
        float rangeCheck = t * t * (3.0 - 2.0 * t);
        float d = samplePos.z - occlusionDepth; // distance from depth to sample
        return (d >= -bias ? 0.0 : rangeCheck);
    }

    void material(inout MaterialInputs material) {
        prepareMaterial(material);

        vec2 uv = variable_vertex.xy; // interpolated to pixel center

        highp float depth = sampleDepthLinear(uv);
        highp vec3 origin = computeViewSpacePositionFromDepth(variable_vertex.zw, depth);
        highp vec3 normal = computeViewSpaceNormalNotNormalized(origin);
        vec3 noise = getNoise(uv);

        // attempt to reject "bad" normals that were reconstructed at an edge that cause
        // false occlusion (black spots)
        // Normal's length should be small before they're normalized, unless they're bad ones.
        if (dot(normal, normal) >= (sq(origin.z * origin.z * 0.000061))) {
            // For now we assume no occlusion, which is wrong in some case, but overall seem to
            // look better.  Maybe those could be handled in the blur pass instead?
            material.baseColor.r = 1.0;
            return;
        }

        normal = normalize(normal);
        float occlusion = 0.0;
        for (uint i = 0u; i < kSphereSampleCount; i++) {
            occlusion += computeAmbientOcclusionSSAO(origin, normal, noise, kSphereSamples[i]);
        }
        float ao = 1.0 - occlusion / float(kSphereSampleCount);

        // simulate user-controled ao^n with n[1, 2]
        ao = mix(ao, ao * ao, materialParams.power);
        material.baseColor.r = ao;
    }
}
